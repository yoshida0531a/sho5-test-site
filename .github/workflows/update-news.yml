name: Update News with AI Summary

on:
  schedule:
    - cron: '0 8 * * *'  # æ¯æœ8æ™‚ã«å®Ÿè¡Œ
  workflow_dispatch:

jobs:
  update-news:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v3
        
      - name: Generate AI-powered news from RSS and sites
        env:
          HF_API_KEY: ${{ secrets.HF_API_KEY }}
        run: |
          python3 -c "
          import json, requests, os, re, hashlib
          from datetime import datetime, timedelta
          import xml.etree.ElementTree as ET
          from urllib.parse import urljoin, urlparse
          
          api_key = os.environ.get('HF_API_KEY', '')
          print(f'ğŸ”‘ API Key: {\"âœ…\" if api_key else \"âŒ Missing\"}')
          
          def summarize_with_ai(text):
              if not api_key:
                  sentences = text.split('ã€‚')
                  if len(sentences) >= 2:
                      return sentences[0] + 'ã€‚' + sentences[1][:50] + 'ã€‚' if len(sentences[1]) > 50 else sentences[0] + 'ã€‚' + sentences[1] + 'ã€‚'
                  return text[:200] + '...'
              try:
                  # æ—¥æœ¬èªå¯¾å¿œã®mT5ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                  response = requests.post(
                      'https://api-inference.huggingface.co/models/megagonlabs/t5-base-japanese-web-8k',
                      headers={'Authorization': f'Bearer {api_key}'},
                      json={'inputs': f'è¦ç´„: {text[:500]}', 'parameters': {'max_length': 200, 'min_length': 80}},
                      timeout=45
                  )
                  if response.status_code == 200:
                      result = response.json()
                      summary = result[0].get('generated_text', '').strip()
                      if summary and len(summary) > 20:
                          return summary
                  print(f'âš ï¸ API Error: {response.status_code}')
                  # APIã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ—¥æœ¬èªã§ã‚·ãƒ³ãƒ—ãƒ«ãªè¦ç´„ã‚’ä½œæˆ
                  sentences = text.split('ã€‚')
                  if len(sentences) >= 2:
                      return sentences[0] + 'ã€‚' + sentences[1][:50] + 'ã€‚' if len(sentences[1]) > 50 else sentences[0] + 'ã€‚' + sentences[1] + 'ã€‚'
                  return text[:200] + '...'
              except Exception as e:
                  print(f'âš ï¸ AI Error: {e}')
                  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ—¥æœ¬èªã§ã‚·ãƒ³ãƒ—ãƒ«ãªè¦ç´„ã‚’ä½œæˆ
                  sentences = text.split('ã€‚')
                  if len(sentences) >= 2:
                      return sentences[0] + 'ã€‚' + sentences[1][:50] + 'ã€‚' if len(sentences[1]) > 50 else sentences[0] + 'ã€‚' + sentences[1] + 'ã€‚'
                  return text[:200] + '...'
          
          def load_previous_data():
              '''å‰å›ã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿'''
              try:
                  with open('news-data.json', 'r', encoding='utf-8') as f:
                      return json.load(f)
              except:
                  return []
          
          def fetch_rss_feeds():
              '''RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—'''
              feeds = []
              try:
                  with open('news-updater/feeds.txt', 'r', encoding='utf-8') as f:
                      for line in f:
                          line = line.strip()
                          if line and not line.startswith('#'):
                              feeds.append(line)
              except:
                  print('âš ï¸ feeds.txt not found')
                  return []
              
              articles = []
              for feed_url in feeds:
                  try:
                      print(f'ğŸ“¡ Fetching RSS: {feed_url}')
                      response = requests.get(feed_url, timeout=10)
                      response.raise_for_status()
                      
                      root = ET.fromstring(response.content)
                      
                      # RSS 2.0 format
                      for item in root.findall('.//item'):
                          title = item.find('title')
                          description = item.find('description')
                          link = item.find('link')
                          pub_date = item.find('pubDate')
                          
                          if title is not None and description is not None:
                              # 24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®ã¿å–å¾—
                              if pub_date is not None:
                                  try:
                                      from email.utils import parsedate_to_datetime
                                      article_date = parsedate_to_datetime(pub_date.text)
                                      if article_date < datetime.now() - timedelta(days=1):
                                          continue
                                  except:
                                      pass
                              
                              # HTMLã‚¿ã‚°ã‚’é™¤å»
                              desc_text = re.sub(r'<[^>]+>', '', description.text or '')
                              
                              articles.append({
                                  'title': title.text or '',
                                  'content': desc_text[:800],
                                  'link': link.text or '' if link is not None else '',
                                  'source': 'RSS',
                                  'feed_url': feed_url
                              })
                  except Exception as e:
                      print(f'âš ï¸ RSS Error {feed_url}: {e}')
              
              return articles
          
          def generate_article_hash(title, content):
              '''è¨˜äº‹ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰'''
              return hashlib.md5(f'{title}{content}'.encode()).hexdigest()
          
          # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
          previous_data = load_previous_data()
          previous_hashes = set()
          for item in previous_data:
              if 'hash' in item:
                  previous_hashes.add(item['hash'])
          
          # RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
          rss_articles = fetch_rss_feeds()
          
          # æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
          new_articles = []
          for article in rss_articles:
              article_hash = generate_article_hash(article['title'], article['content'])
              if article_hash not in previous_hashes:
                  new_articles.append(article)
          
          print(f'ğŸ“Š Found {len(rss_articles)} RSS articles, {len(new_articles)} new articles')
          
          # æ–°ã—ã„è¨˜äº‹ã‚’å‡¦ç†
          processed = []
          for article in new_articles[:10]:  # æœ€å¤§10è¨˜äº‹ã¾ã§å‡¦ç†
              try:
                  summary = summarize_with_ai(article['content'])
                  article_hash = generate_article_hash(article['title'], article['content'])
                  
                  processed.append({
                      'title': article['title'],
                      'summary': summary,
                      'link': article['link'],
                      'source': article['source'],
                      'pubDate': datetime.now().isoformat(),
                      'processed_at': datetime.now().isoformat(),
                      'hash': article_hash
                  })
                  print(f'âœ… Processed: {article[\"title\"][:50]}...')
              except Exception as e:
                  print(f'âš ï¸ Processing error: {e}')
          
          # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆæœ€æ–°30ä»¶ã¾ã§ä¿æŒï¼‰
          all_articles = processed + previous_data
          all_articles = all_articles[:30]
          
          with open('news-data.json', 'w', encoding='utf-8') as f:
              json.dump(all_articles, f, ensure_ascii=False, indent=2)
          
          print(f'âœ… Total articles saved: {len(all_articles)} (New: {len(processed)})')
          "
          
      - name: Create HTML
        run: |
          echo "Creating HTML from AI data..."
          python3 -c "
          import json
          from datetime import datetime
          
          # Load AI-generated data
          with open('news-data.json', 'r', encoding='utf-8') as f:
              news_data = json.load(f)
          
          # Generate HTML with AI summaries
          html = '''<!DOCTYPE html>
          <html lang=\"ja\">
          <head>
            <meta charset=\"UTF-8\">
            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">
            <title>Newsroom - Shogo Fun Site</title>
            <style>
              * { margin: 0; padding: 0; box-sizing: border-box; }
              body { 
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: #333;
                line-height: 1.6;
                min-height: 100vh;
              }
              .container {
                max-width: 1200px;
                margin: 0 auto;
                padding: 20px;
              }
              header { 
                text-align: center; 
                margin-bottom: 40px;
                background: rgba(255, 255, 255, 0.95);
                padding: 30px;
                border-radius: 15px;
                box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
                backdrop-filter: blur(10px);
              }
              .header-title {
                font-size: 2.5rem;
                font-weight: 700;
                color: #2c3e50;
                margin-bottom: 10px;
              }
              .header-subtitle {
                font-size: 1.1rem;
                color: #7f8c8d;
                margin-bottom: 15px;
              }
              .last-update {
                font-size: 0.9rem;
                color: #95a5a6;
                font-style: italic;
              }
              .news-grid {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
                gap: 25px;
                margin-bottom: 40px;
              }
              .news-card {
                background: rgba(255, 255, 255, 0.95);
                border-radius: 15px;
                padding: 25px;
                box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
                backdrop-filter: blur(10px);
                transition: all 0.3s ease;
                border: 1px solid rgba(255, 255, 255, 0.2);
              }
              .news-card:hover {
                transform: translateY(-5px);
                box-shadow: 0 15px 40px rgba(0, 0, 0, 0.15);
              }
              .news-title {
                font-size: 1.3rem;
                font-weight: 600;
                margin-bottom: 15px;
                line-height: 1.4;
              }
              .news-title a {
                color: #2c3e50;
                text-decoration: none;
                transition: color 0.3s ease;
              }
              .news-title a:hover {
                color: #3498db;
              }
              .news-meta {
                display: flex;
                justify-content: space-between;
                align-items: center;
                margin-bottom: 15px;
                font-size: 0.85rem;
                color: #7f8c8d;
              }
              .news-source {
                background: #3498db;
                color: white;
                padding: 4px 12px;
                border-radius: 20px;
                font-size: 0.8rem;
                font-weight: 500;
              }
              .news-date {
                font-style: italic;
              }
              .news-summary {
                font-size: 1rem;
                line-height: 1.6;
                color: #34495e;
                margin-bottom: 15px;
              }
              .read-more {
                display: inline-block;
                color: #3498db;
                text-decoration: none;
                font-weight: 500;
                font-size: 0.9rem;
                transition: color 0.3s ease;
              }
              .read-more:hover {
                color: #2980b9;
              }
              .stats {
                text-align: center;
                background: rgba(255, 255, 255, 0.95);
                padding: 20px;
                border-radius: 15px;
                box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
                backdrop-filter: blur(10px);
                margin-bottom: 30px;
              }
              .stats-text {
                font-size: 1rem;
                color: #7f8c8d;
              }
              footer { 
                text-align: center; 
                color: rgba(255, 255, 255, 0.8);
                font-size: 0.9rem;
                margin-top: 40px;
              }
              @media (max-width: 768px) {
                .container { padding: 15px; }
                .header-title { font-size: 2rem; }
                .news-grid { grid-template-columns: 1fr; gap: 20px; }
                .news-card { padding: 20px; }
                .news-title { font-size: 1.2rem; }
              }
            </style>
          </head>
          <body>
            <div class=\"container\">
              <header>
                <div class=\"header-title\">ğŸ“° Newsroom</div>
                <div class=\"header-subtitle\">AI-powered news summaries updated daily</div>
                <div class=\"last-update\">Last updated: ''' + datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M') + '''</div>
              </header>'''
          
          # Add stats section
          total_articles = len(news_data)
          html += f'''
              <div class=\"stats\">
                <div class=\"stats-text\">ğŸ“Š ç¾åœ¨ {total_articles} ä»¶ã®è¨˜äº‹ã‚’è¡¨ç¤ºä¸­ | ğŸ¤– AIè¦ç´„ã§åŠ¹ç‡çš„ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯</div>
              </div>
              
              <div class=\"news-grid\">'''
          
          # Add AI-generated news items
          for item in news_data:
              pub_date = datetime.fromisoformat(item['pubDate']).strftime('%mæœˆ%dæ—¥ %H:%M')
              source = item.get('source', 'RSS')
              
              # Truncate summary for better display
              summary = item['summary']
              if len(summary) > 150:
                  summary = summary[:150] + '...'
              
              html += f'''
                <div class=\"news-card\">
                  <div class=\"news-title\">
                    <a href=\"{item['link']}\" target=\"_blank\">{item['title']}</a>
                  </div>
                  <div class=\"news-meta\">
                    <span class=\"news-source\">{source}</span>
                    <span class=\"news-date\">{pub_date}</span>
                  </div>
                  <div class=\"news-summary\">{summary}</div>
                  <a href=\"{item['link']}\" target=\"_blank\" class=\"read-more\">ç¶šãã‚’èª­ã‚€ â†’</a>
                </div>'''
          
          html += '''
              </div>
              
              <footer>
                <div>ğŸš€ Powered by AI | Copyright Â© 2023-2025 Akira Yoshida</div>
                <div style=\"margin-top: 10px; font-size: 0.8rem;\">
                  Sources: NHK, Yahoo News, ITmedia, TechCrunch, GIGAZINE, ASCII.jp
                </div>
              </footer>
            </div>
          </body>
          </html>'''
          
          # Write HTML file
          with open('news.html', 'w', encoding='utf-8') as f:
              f.write(html)
          
          print(f'âœ“ Created HTML with {len(news_data)} AI-generated articles')
          "
          echo "âœ“ HTML generation completed"
          
      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add news.html news-data.json
          
          if ! git diff --staged --quiet; then
            git commit -m "Update news ğŸ¤–"
            git push
          else
            echo "No changes"
          fi