name: Update News with AI Summary

on:
  schedule:
    - cron: '0 8 * * *'  # 毎朝8時に実行
  workflow_dispatch:

jobs:
  update-news:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v3
        
      - name: Generate AI-powered news from RSS and sites
        env:
          HF_API_KEY: ${{ secrets.HF_API_KEY }}
        run: |
          python3 -c "
          import json, requests, os, re, hashlib
          from datetime import datetime, timedelta
          import xml.etree.ElementTree as ET
          from urllib.parse import urljoin, urlparse
          
          api_key = os.environ.get('HF_API_KEY', '')
          print(f'🔑 API Key: {\"✅\" if api_key else \"❌ Missing\"}')
          
          def summarize_with_ai(text):
              if not api_key:
                  sentences = text.split('。')
                  if len(sentences) >= 2:
                      return sentences[0] + '。' + sentences[1][:50] + '。' if len(sentences[1]) > 50 else sentences[0] + '。' + sentences[1] + '。'
                  return text[:200] + '...'
              try:
                  # 日本語対応のmT5モデルを使用
                  response = requests.post(
                      'https://api-inference.huggingface.co/models/megagonlabs/t5-base-japanese-web-8k',
                      headers={'Authorization': f'Bearer {api_key}'},
                      json={'inputs': f'要約: {text[:500]}', 'parameters': {'max_length': 200, 'min_length': 80}},
                      timeout=45
                  )
                  if response.status_code == 200:
                      result = response.json()
                      summary = result[0].get('generated_text', '').strip()
                      if summary and len(summary) > 20:
                          return summary
                  print(f'⚠️ API Error: {response.status_code}')
                  # APIエラーの場合は日本語でシンプルな要約を作成
                  sentences = text.split('。')
                  if len(sentences) >= 2:
                      return sentences[0] + '。' + sentences[1][:50] + '。' if len(sentences[1]) > 50 else sentences[0] + '。' + sentences[1] + '。'
                  return text[:200] + '...'
              except Exception as e:
                  print(f'⚠️ AI Error: {e}')
                  # エラーの場合は日本語でシンプルな要約を作成
                  sentences = text.split('。')
                  if len(sentences) >= 2:
                      return sentences[0] + '。' + sentences[1][:50] + '。' if len(sentences[1]) > 50 else sentences[0] + '。' + sentences[1] + '。'
                  return text[:200] + '...'
          
          def load_previous_data():
              '''前回の記事データを読み込み'''
              try:
                  with open('news-data.json', 'r', encoding='utf-8') as f:
                      return json.load(f)
              except:
                  return []
          
          def check_tanibata_keywords(text):
              '''谷端選手関連のキーワードをチェック'''
              if '谷端' in text:
                  return True, '谷端'
              return False, None
          
          def fetch_rss_feeds():
              '''RSS フィードから記事を取得'''
              feeds = []
              try:
                  with open('news-updater/feeds.txt', 'r', encoding='utf-8') as f:
                      for line in f:
                          line = line.strip()
                          if line and not line.startswith('#'):
                              feeds.append(line)
              except:
                  print('⚠️ feeds.txt not found')
                  return []
              
              articles = []
              for feed_url in feeds:
                  try:
                      print(f'📡 Fetching RSS: {feed_url}')
                      response = requests.get(feed_url, timeout=15)
                      response.raise_for_status()
                      
                      root = ET.fromstring(response.content)
                      
                      # RSS 2.0 format
                      for item in root.findall('.//item'):
                          title = item.find('title')
                          description = item.find('description')
                          link = item.find('link')
                          pub_date = item.find('pubDate')
                          
                          if title is not None and description is not None:
                              title_text = title.text or ''
                              desc_text = re.sub(r'<[^>]+>', '', description.text or '')
                              full_text = title_text + ' ' + desc_text
                              
                              # 谷端選手関連のキーワードチェック
                              has_keyword, found_keyword = check_tanibata_keywords(full_text)
                              if not has_keyword:
                                  continue
                              
                              # 48時間以内の記事のみ取得
                              if pub_date is not None:
                                  try:
                                      from email.utils import parsedate_to_datetime
                                      article_date = parsedate_to_datetime(pub_date.text)
                                      if article_date < datetime.now() - timedelta(days=2):
                                          continue
                                  except:
                                      pass
                              
                              articles.append({
                                  'title': title_text,
                                  'content': desc_text[:800],
                                  'link': link.text or '' if link is not None else '',
                                  'source': 'RSS',
                                  'feed_url': feed_url,
                                  'keyword': found_keyword
                              })
                              print(f'⚾ Found 谷端 RSS article: {title_text[:50]}...')
                  except Exception as e:
                      print(f'⚠️ RSS Error {feed_url}: {e}')
              
              return articles
          
          def fetch_news_sites():
              '''ニュースサイトから記事を直接取得'''
              sites = []
              try:
                  with open('news-updater/news_sites.txt', 'r', encoding='utf-8') as f:
                      for line in f:
                          line = line.strip()
                          if line and not line.startswith('#'):
                              sites.append(line)
              except:
                  print('⚠️ news_sites.txt not found')
                  return []
              
              articles = []
              for site_url in sites:
                  try:
                      print(f'🌐 Fetching website: {site_url}')
                      response = requests.get(site_url, timeout=15, headers={
                          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                      })
                      response.raise_for_status()
                      
                      # HTMLコンテンツから記事を抽出
                      html_content = response.text
                      
                      # 基本的なHTMLパース（タイトルとコンテンツを抽出）
                      title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
                      title_text = title_match.group(1).strip() if title_match else site_url
                      
                      # メインコンテンツを抽出（記事本文）
                      content_text = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
                      content_text = re.sub(r'<style[^>]*>.*?</style>', '', content_text, flags=re.DOTALL | re.IGNORECASE)
                      content_text = re.sub(r'<[^>]+>', ' ', content_text)
                      content_text = re.sub(r'\\s+', ' ', content_text).strip()
                      
                      full_text = title_text + ' ' + content_text
                      
                      # 谷端選手関連のキーワードチェック
                      has_keyword, found_keyword = check_tanibata_keywords(full_text)
                      if not has_keyword:
                          continue
                      
                      articles.append({
                          'title': title_text,
                          'content': content_text[:1000],
                          'link': site_url,
                          'source': 'Website',
                          'site_url': site_url,
                          'keyword': found_keyword
                      })
                      print(f'⚾ Found 谷端 website article: {title_text[:50]}...')
                      
                  except Exception as e:
                      print(f'⚠️ Website Error {site_url}: {e}')
              
              return articles
          
          def generate_article_hash(title, content):
              '''記事のハッシュ値を生成（重複チェック用）'''
              return hashlib.md5(f'{title}{content}'.encode()).hexdigest()
          
          # 前回のデータを読み込み
          previous_data = load_previous_data()
          previous_hashes = set()
          for item in previous_data:
              if 'hash' in item:
                  previous_hashes.add(item['hash'])
          
          # RSS フィードと直接サイトから記事を取得
          rss_articles = fetch_rss_feeds()
          site_articles = fetch_news_sites()
          
          # 全記事を結合
          all_fetched_articles = rss_articles + site_articles
          
          # 新しい記事のみをフィルタリング
          new_articles = []
          for article in all_fetched_articles:
              article_hash = generate_article_hash(article['title'], article['content'])
              if article_hash not in previous_hashes:
                  new_articles.append(article)
          
          print(f'📊 Found {len(rss_articles)} RSS articles, {len(site_articles)} website articles')
          print(f'📊 Total: {len(all_fetched_articles)} articles, {len(new_articles)} new articles')
          
          # 谷端関連記事がない場合はページ更新をスキップ
          if len(new_articles) == 0:
              print('🔍 No new 谷端 articles found. Skipping page update.')
              exit(0)
          
          # 新しい記事を処理
          processed = []
          for article in new_articles[:5]:  # 最大5記事まで処理
              try:
                  summary = summarize_with_ai(article['content'])
                  article_hash = generate_article_hash(article['title'], article['content'])
                  
                  processed.append({
                      'title': article['title'],
                      'summary': summary,
                      'link': article['link'],
                      'source': article['source'],
                      'pubDate': datetime.now().isoformat(),
                      'processed_at': datetime.now().isoformat(),
                      'hash': article_hash
                  })
                  print(f'✅ Processed: {article[\"title\"][:50]}...')
              except Exception as e:
                  print(f'⚠️ Processing error: {e}')
          
          # 前回のデータと新しいデータを結合（最新20件まで保持）
          all_articles = processed + previous_data
          all_articles = all_articles[:20]
          
          with open('news-data.json', 'w', encoding='utf-8') as f:
              json.dump(all_articles, f, ensure_ascii=False, indent=2)
          
          print(f'✅ Total articles saved: {len(all_articles)} (New: {len(processed)})')
          "
          
      - name: Create HTML
        run: |
          echo "Creating HTML from AI data..."
          python3 -c "
          import json
          from datetime import datetime
          
          # Load AI-generated data
          with open('news-data.json', 'r', encoding='utf-8') as f:
              news_data = json.load(f)
          
          # Generate HTML with AI summaries
          html = '''<!DOCTYPE html>
          <html lang=\"ja\">
          <head>
            <meta charset=\"UTF-8\">
            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">
            <title>Newsroom - Shogo Fun Site</title>
            <style>
              * { margin: 0; padding: 0; box-sizing: border-box; }
              body { font-family: sans-serif; background: white; color: black; }
              header { text-align: center; font-size: 28px; font-weight: bold; margin: 20px 0; }
              .news-feed { width: calc(100% - 20px); max-width: 800px; margin: 0 auto; padding: 20px; }
              .news-item { border-bottom: 1px solid #ddd; margin-bottom: 20px; padding-bottom: 15px; }
              .news-item h3 { margin: 5px 0; font-size: 18px; }
              .news-item h3 a { color: black; text-decoration: none; }
              .news-item .date { margin: 5px 0; font-size: 12px; color: #666; }
              .news-item .summary { margin: 10px 0; font-size: 14px; line-height: 1.5; }
              footer { text-align: center; font-size: 12px; margin: 20px 0; }
            </style>
          </head>
          <body>
            <header>Newsroom</header>
            <section class=\"news-feed\">'''
          
          # Add AI-generated news items
          for item in news_data:
              pub_date = datetime.fromisoformat(item['pubDate']).strftime('%Y年%m月%d日')
              
              html += f'''
              <div class=\"news-item\">
                <h3><a href=\"{item['link']}\">{item['title']}</a></h3>
                <p class=\"date\">{pub_date}</p>
                <p class=\"summary\">{item['summary']}</p>
              </div>'''
          
          html += '''
            </section>
            <footer>Copyright © 2023-2025 Akira Yoshida.</footer>
          </body>
          </html>'''
          
          # Write HTML file
          with open('news.html', 'w', encoding='utf-8') as f:
              f.write(html)
          
          print(f'✓ Created HTML with {len(news_data)} AI-generated articles')
          "
          echo "✓ HTML generation completed"
          
      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add news.html news-data.json
          
          if ! git diff --staged --quiet; then
            git commit -m "Update news 🤖"
            git push
          else
            echo "No changes"
          fi