name: Update News with AI Summary

on:
  schedule:
    - cron: '0 8 * * *'  # æ¯æœ8æ™‚ã«å®Ÿè¡Œ
  workflow_dispatch:

jobs:
  update-news:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v3
        
      - name: Generate AI-powered news from RSS and sites
        env:
          HF_API_KEY: ${{ secrets.HF_API_KEY }}
        run: |
          python3 -c "
          import json, requests, os, re, hashlib
          from datetime import datetime, timedelta
          import xml.etree.ElementTree as ET
          from urllib.parse import urljoin, urlparse
          
          api_key = os.environ.get('HF_API_KEY', '')
          print(f'ğŸ”‘ API Key: {\"âœ…\" if api_key else \"âŒ Missing\"}')
          
          def summarize_with_ai(text):
              if not api_key:
                  sentences = text.split('ã€‚')
                  if len(sentences) >= 2:
                      return sentences[0] + 'ã€‚' + sentences[1][:50] + 'ã€‚' if len(sentences[1]) > 50 else sentences[0] + 'ã€‚' + sentences[1] + 'ã€‚'
                  return text[:200] + '...'
              try:
                  # æ—¥æœ¬èªå¯¾å¿œã®mT5ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                  response = requests.post(
                      'https://api-inference.huggingface.co/models/megagonlabs/t5-base-japanese-web-8k',
                      headers={'Authorization': f'Bearer {api_key}'},
                      json={'inputs': f'è¦ç´„: {text[:500]}', 'parameters': {'max_length': 200, 'min_length': 80}},
                      timeout=45
                  )
                  if response.status_code == 200:
                      result = response.json()
                      summary = result[0].get('generated_text', '').strip()
                      if summary and len(summary) > 20:
                          return summary
                  print(f'âš ï¸ API Error: {response.status_code}')
                  # APIã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ—¥æœ¬èªã§ã‚·ãƒ³ãƒ—ãƒ«ãªè¦ç´„ã‚’ä½œæˆ
                  sentences = text.split('ã€‚')
                  if len(sentences) >= 2:
                      return sentences[0] + 'ã€‚' + sentences[1][:50] + 'ã€‚' if len(sentences[1]) > 50 else sentences[0] + 'ã€‚' + sentences[1] + 'ã€‚'
                  return text[:200] + '...'
              except Exception as e:
                  print(f'âš ï¸ AI Error: {e}')
                  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ—¥æœ¬èªã§ã‚·ãƒ³ãƒ—ãƒ«ãªè¦ç´„ã‚’ä½œæˆ
                  sentences = text.split('ã€‚')
                  if len(sentences) >= 2:
                      return sentences[0] + 'ã€‚' + sentences[1][:50] + 'ã€‚' if len(sentences[1]) > 50 else sentences[0] + 'ã€‚' + sentences[1] + 'ã€‚'
                  return text[:200] + '...'
          
          def load_previous_data():
              '''å‰å›ã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿'''
              try:
                  with open('news-data.json', 'r', encoding='utf-8') as f:
                      return json.load(f)
              except:
                  return []
          
          def check_tanibata_keywords(text):
              '''è°·ç«¯é¸æ‰‹é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯'''
              if 'è°·ç«¯' in text:
                  return True, 'è°·ç«¯'
              return False, None
          
          def fetch_rss_feeds():
              '''RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—'''
              feeds = []
              try:
                  with open('news-updater/feeds.txt', 'r', encoding='utf-8') as f:
                      for line in f:
                          line = line.strip()
                          if line and not line.startswith('#'):
                              feeds.append(line)
              except:
                  print('âš ï¸ feeds.txt not found')
                  return []
              
              articles = []
              for feed_url in feeds:
                  try:
                      print(f'ğŸ“¡ Fetching RSS: {feed_url}')
                      response = requests.get(feed_url, timeout=15)
                      response.raise_for_status()
                      
                      root = ET.fromstring(response.content)
                      
                      # RSS 2.0 format
                      for item in root.findall('.//item'):
                          title = item.find('title')
                          description = item.find('description')
                          link = item.find('link')
                          pub_date = item.find('pubDate')
                          
                          if title is not None and description is not None:
                              title_text = title.text or ''
                              desc_text = re.sub(r'<[^>]+>', '', description.text or '')
                              full_text = title_text + ' ' + desc_text
                              
                              # è°·ç«¯é¸æ‰‹é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                              has_keyword, found_keyword = check_tanibata_keywords(full_text)
                              if not has_keyword:
                                  continue
                              
                              # 48æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®ã¿å–å¾—
                              if pub_date is not None:
                                  try:
                                      from email.utils import parsedate_to_datetime
                                      article_date = parsedate_to_datetime(pub_date.text)
                                      if article_date < datetime.now() - timedelta(days=2):
                                          continue
                                  except:
                                      pass
                              
                              articles.append({
                                  'title': title_text,
                                  'content': desc_text[:800],
                                  'link': link.text or '' if link is not None else '',
                                  'source': 'RSS',
                                  'feed_url': feed_url,
                                  'keyword': found_keyword
                              })
                              print(f'âš¾ Found è°·ç«¯ RSS article: {title_text[:50]}...')
                  except Exception as e:
                      print(f'âš ï¸ RSS Error {feed_url}: {e}')
              
              return articles
          
          def fetch_news_sites():
              '''ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰è¨˜äº‹ã‚’ç›´æ¥å–å¾—'''
              sites = []
              try:
                  with open('news-updater/news_sites.txt', 'r', encoding='utf-8') as f:
                      for line in f:
                          line = line.strip()
                          if line and not line.startswith('#'):
                              sites.append(line)
              except:
                  print('âš ï¸ news_sites.txt not found')
                  return []
              
              articles = []
              for site_url in sites:
                  try:
                      print(f'ğŸŒ Fetching website: {site_url}')
                      response = requests.get(site_url, timeout=15, headers={
                          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                      })
                      response.raise_for_status()
                      
                      # HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰è¨˜äº‹ã‚’æŠ½å‡º
                      html_content = response.text
                      
                      # åŸºæœ¬çš„ãªHTMLãƒ‘ãƒ¼ã‚¹ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼‰
                      title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
                      title_text = title_match.group(1).strip() if title_match else site_url
                      
                      # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆè¨˜äº‹æœ¬æ–‡ï¼‰
                      content_text = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
                      content_text = re.sub(r'<style[^>]*>.*?</style>', '', content_text, flags=re.DOTALL | re.IGNORECASE)
                      content_text = re.sub(r'<[^>]+>', ' ', content_text)
                      content_text = re.sub(r'\\s+', ' ', content_text).strip()
                      
                      full_text = title_text + ' ' + content_text
                      
                      # è°·ç«¯é¸æ‰‹é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                      has_keyword, found_keyword = check_tanibata_keywords(full_text)
                      if not has_keyword:
                          continue
                      
                      articles.append({
                          'title': title_text,
                          'content': content_text[:1000],
                          'link': site_url,
                          'source': 'Website',
                          'site_url': site_url,
                          'keyword': found_keyword
                      })
                      print(f'âš¾ Found è°·ç«¯ website article: {title_text[:50]}...')
                      
                  except Exception as e:
                      print(f'âš ï¸ Website Error {site_url}: {e}')
              
              return articles
          
          def generate_article_hash(title, content):
              '''è¨˜äº‹ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰'''
              return hashlib.md5(f'{title}{content}'.encode()).hexdigest()
          
          # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
          previous_data = load_previous_data()
          previous_hashes = set()
          for item in previous_data:
              if 'hash' in item:
                  previous_hashes.add(item['hash'])
          
          # RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã¨ç›´æ¥ã‚µã‚¤ãƒˆã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
          rss_articles = fetch_rss_feeds()
          site_articles = fetch_news_sites()
          
          # å…¨è¨˜äº‹ã‚’çµåˆ
          all_fetched_articles = rss_articles + site_articles
          
          # æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
          new_articles = []
          for article in all_fetched_articles:
              article_hash = generate_article_hash(article['title'], article['content'])
              if article_hash not in previous_hashes:
                  new_articles.append(article)
          
          print(f'ğŸ“Š Found {len(rss_articles)} RSS articles, {len(site_articles)} website articles')
          print(f'ğŸ“Š Total: {len(all_fetched_articles)} articles, {len(new_articles)} new articles')
          
          # è°·ç«¯é–¢é€£è¨˜äº‹ãŒãªã„å ´åˆã¯ãƒšãƒ¼ã‚¸æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—
          if len(new_articles) == 0:
              print('ğŸ” No new è°·ç«¯ articles found. Skipping page update.')
              exit(0)
          
          # æ–°ã—ã„è¨˜äº‹ã‚’å‡¦ç†
          processed = []
          for article in new_articles[:5]:  # æœ€å¤§5è¨˜äº‹ã¾ã§å‡¦ç†
              try:
                  summary = summarize_with_ai(article['content'])
                  article_hash = generate_article_hash(article['title'], article['content'])
                  
                  processed.append({
                      'title': article['title'],
                      'summary': summary,
                      'link': article['link'],
                      'source': article['source'],
                      'pubDate': datetime.now().isoformat(),
                      'processed_at': datetime.now().isoformat(),
                      'hash': article_hash
                  })
                  print(f'âœ… Processed: {article[\"title\"][:50]}...')
              except Exception as e:
                  print(f'âš ï¸ Processing error: {e}')
          
          # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆæœ€æ–°20ä»¶ã¾ã§ä¿æŒï¼‰
          all_articles = processed + previous_data
          all_articles = all_articles[:20]
          
          with open('news-data.json', 'w', encoding='utf-8') as f:
              json.dump(all_articles, f, ensure_ascii=False, indent=2)
          
          print(f'âœ… Total articles saved: {len(all_articles)} (New: {len(processed)})')
          "
          
      - name: Create HTML
        run: |
          echo "Creating HTML from AI data..."
          python3 -c "
          import json
          from datetime import datetime
          
          # Load AI-generated data
          with open('news-data.json', 'r', encoding='utf-8') as f:
              news_data = json.load(f)
          
          # Generate HTML with AI summaries
          html = '''<!DOCTYPE html>
          <html lang=\"ja\">
          <head>
            <meta charset=\"UTF-8\">
            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">
            <title>Newsroom - Shogo Fun Site</title>
            <style>
              * { margin: 0; padding: 0; box-sizing: border-box; }
              body { font-family: sans-serif; background: white; color: black; }
              header { text-align: center; font-size: 28px; font-weight: bold; margin: 20px 0; }
              .news-feed { width: calc(100% - 20px); max-width: 800px; margin: 0 auto; padding: 20px; }
              .news-item { border-bottom: 1px solid #ddd; margin-bottom: 20px; padding-bottom: 15px; }
              .news-item h3 { margin: 5px 0; font-size: 18px; }
              .news-item h3 a { color: black; text-decoration: none; }
              .news-item .date { margin: 5px 0; font-size: 12px; color: #666; }
              .news-item .summary { margin: 10px 0; font-size: 14px; line-height: 1.5; }
              footer { text-align: center; font-size: 12px; margin: 20px 0; }
            </style>
          </head>
          <body>
            <header>Newsroom</header>
            <section class=\"news-feed\">'''
          
          # Add AI-generated news items
          for item in news_data:
              pub_date = datetime.fromisoformat(item['pubDate']).strftime('%Yå¹´%mæœˆ%dæ—¥')
              
              html += f'''
              <div class=\"news-item\">
                <h3><a href=\"{item['link']}\">{item['title']}</a></h3>
                <p class=\"date\">{pub_date}</p>
                <p class=\"summary\">{item['summary']}</p>
              </div>'''
          
          html += '''
            </section>
            <footer>Copyright Â© 2023-2025 Akira Yoshida.</footer>
          </body>
          </html>'''
          
          # Write HTML file
          with open('news.html', 'w', encoding='utf-8') as f:
              f.write(html)
          
          print(f'âœ“ Created HTML with {len(news_data)} AI-generated articles')
          "
          echo "âœ“ HTML generation completed"
          
      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add news.html news-data.json
          
          if ! git diff --staged --quiet; then
            git commit -m "Update news ğŸ¤–"
            git push
          else
            echo "No changes"
          fi