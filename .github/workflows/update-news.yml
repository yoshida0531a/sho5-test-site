name: Update News with AI Summary

on:
  schedule:
    - cron: '0 8 * * *'  # æ¯æœ8æ™‚ã«å®Ÿè¡Œ
  workflow_dispatch:

jobs:
  update-news:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v3
        
      - name: Generate AI-powered news from RSS and sites
        env:
          HF_API_KEY: ${{ secrets.HF_API_KEY }}
        run: |
          python3 -c "
          import json, requests, os, re, hashlib
          from datetime import datetime, timedelta
          import xml.etree.ElementTree as ET
          from urllib.parse import urljoin, urlparse
          
          api_key = os.environ.get('HF_API_KEY', '')
          print(f'ğŸ”‘ API Key: {\"âœ…\" if api_key else \"âŒ Missing\"}')
          
          def summarize_with_ai(text, is_baseball=True):
              # é‡çƒå°‚ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
              baseball_context = 'é‡çƒé–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚' if is_baseball else ''
              
              if not api_key:
                  sentences = text.split('ã€‚')
                  if len(sentences) >= 2:
                      return sentences[0] + 'ã€‚' + sentences[1][:50] + 'ã€‚' if len(sentences[1]) > 50 else sentences[0] + 'ã€‚' + sentences[1] + 'ã€‚'
                  return text[:200] + '...'
              try:
                  # æ—¥æœ¬èªå¯¾å¿œã®mT5ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆé‡çƒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãï¼‰
                  prompt = f'{baseball_context} è¦ç´„: {text[:600]}'
                  response = requests.post(
                      'https://api-inference.huggingface.co/models/megagonlabs/t5-base-japanese-web-8k',
                      headers={'Authorization': f'Bearer {api_key}'},
                      json={'inputs': prompt, 'parameters': {'max_length': 250, 'min_length': 100}},
                      timeout=45
                  )
                  if response.status_code == 200:
                      result = response.json()
                      summary = result[0].get('generated_text', '').strip()
                      if summary and len(summary) > 30:
                          return summary
                  print(f'âš ï¸ API Error: {response.status_code}')
                  # APIã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é‡çƒé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¿æŒã—ã¦è¦ç´„
                  sentences = text.split('ã€‚')
                  filtered_sentences = []
                  for sentence in sentences:
                      if 'è°·ç«¯' in sentence or 'é‡çƒ' in sentence or 'è©¦åˆ' in sentence:
                          filtered_sentences.append(sentence)
                      elif len(filtered_sentences) < 2:
                          filtered_sentences.append(sentence)
                  
                  if len(filtered_sentences) >= 2:
                      return filtered_sentences[0] + 'ã€‚' + (filtered_sentences[1][:60] + 'ã€‚' if len(filtered_sentences[1]) > 60 else filtered_sentences[1] + 'ã€‚')
                  return text[:200] + '...'
              except Exception as e:
                  print(f'âš ï¸ AI Error: {e}')
                  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é‡çƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆã—ã¦è¦ç´„
                  sentences = text.split('ã€‚')
                  for sentence in sentences:
                      if 'è°·ç«¯' in sentence and len(sentence) > 20:
                          return sentence + 'ã€‚'
                  return text[:200] + '...'
          
          def load_previous_data():
              '''å‰å›ã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿'''
              try:
                  with open('news-data.json', 'r', encoding='utf-8') as f:
                      return json.load(f)
              except:
                  return []
          
          def check_tanibata_keywords(text):
              '''è°·ç«¯é¸æ‰‹é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯'''
              keywords = [
                  'è°·ç«¯ã€€æ—¥å¤§', 'è°·ç«¯ã€€å¤§å­¦', 'è°·ç«¯ã€€æ˜Ÿç¨œ',
                  'è°·ç«¯ æ—¥å¤§', 'è°·ç«¯ å¤§å­¦', 'è°·ç«¯ æ˜Ÿç¨œ',
                  'è°·ç«¯æ—¥å¤§', 'è°·ç«¯å¤§å­¦', 'è°·ç«¯æ˜Ÿç¨œ',
                  'è°·ç«¯ã€€å·¨äºº', 'è°·ç«¯ã€€ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„',
                  'è°·ç«¯ å·¨äºº', 'è°·ç«¯ ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„'
              ]
              
              for keyword in keywords:
                  if keyword in text:
                      return True, keyword
              return False, None
          
          def fetch_rss_feeds():
              '''RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ï¼ˆè°·ç«¯é¸æ‰‹é–¢é€£ã®ã¿ï¼‰'''
              feeds = []
              try:
                  with open('news-updater/feeds.txt', 'r', encoding='utf-8') as f:
                      for line in f:
                          line = line.strip()
                          if line and not line.startswith('#'):
                              feeds.append(line)
              except:
                  print('âš ï¸ feeds.txt not found')
                  return []
              
              articles = []
              for feed_url in feeds:
                  try:
                      print(f'ğŸ“¡ Fetching RSS: {feed_url}')
                      response = requests.get(feed_url, timeout=15)
                      response.raise_for_status()
                      
                      root = ET.fromstring(response.content)
                      
                      # RSS 2.0 format
                      for item in root.findall('.//item'):
                          title = item.find('title')
                          description = item.find('description')
                          link = item.find('link')
                          pub_date = item.find('pubDate')
                          
                          if title is not None and description is not None:
                              title_text = title.text or ''
                              desc_text = re.sub(r'<[^>]+>', '', description.text or '')
                              full_text = title_text + ' ' + desc_text
                              
                              # è°·ç«¯é¸æ‰‹é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                              has_keyword, found_keyword = check_tanibata_keywords(full_text)
                              if not has_keyword:
                                  continue
                              
                              # 48æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®ã¿å–å¾—ï¼ˆé‡çƒãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯æ›´æ–°é »åº¦ãŒä½ã„ãŸã‚ï¼‰
                              if pub_date is not None:
                                  try:
                                      from email.utils import parsedate_to_datetime
                                      article_date = parsedate_to_datetime(pub_date.text)
                                      if article_date < datetime.now() - timedelta(days=2):
                                          continue
                                  except:
                                      pass
                              
                              articles.append({
                                  'title': title_text,
                                  'content': desc_text[:1000],
                                  'link': link.text or '' if link is not None else '',
                                  'source': 'Baseball RSS',
                                  'feed_url': feed_url,
                                  'keyword': found_keyword,
                                  'category': 'å¤§å­¦é‡çƒ'
                              })
                              print(f'âš¾ Found è°·ç«¯ article: {title_text[:50]}... (keyword: {found_keyword})')
                  except Exception as e:
                      print(f'âš ï¸ RSS Error {feed_url}: {e}')
              
              return articles
          
          def generate_article_hash(title, content):
              '''è¨˜äº‹ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰'''
              return hashlib.md5(f'{title}{content}'.encode()).hexdigest()
          
          # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
          previous_data = load_previous_data()
          previous_hashes = set()
          for item in previous_data:
              if 'hash' in item:
                  previous_hashes.add(item['hash'])
          
          # RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
          rss_articles = fetch_rss_feeds()
          
          # æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
          new_articles = []
          for article in rss_articles:
              article_hash = generate_article_hash(article['title'], article['content'])
              if article_hash not in previous_hashes:
                  new_articles.append(article)
          
          print(f'ğŸ“Š Found {len(rss_articles)} RSS articles, {len(new_articles)} new articles')
          
          # æ–°ã—ã„è¨˜äº‹ã‚’å‡¦ç†
          processed = []
          for article in new_articles[:15]:  # è°·ç«¯é–¢é€£è¨˜äº‹ã¯è²´é‡ãªã®ã§æœ€å¤§15è¨˜äº‹ã¾ã§å‡¦ç†
              try:
                  summary = summarize_with_ai(article['content'], is_baseball=True)
                  article_hash = generate_article_hash(article['title'], article['content'])
                  
                  processed.append({
                      'title': article['title'],
                      'summary': summary,
                      'link': article['link'],
                      'source': article['source'],
                      'category': article['category'],
                      'keyword': article['keyword'],
                      'pubDate': datetime.now().isoformat(),
                      'processed_at': datetime.now().isoformat(),
                      'hash': article_hash
                  })
                  print(f'âš¾ Processed è°·ç«¯ article: {article[\"title\"][:50]}... (keyword: {article[\"keyword\"]})')
              except Exception as e:
                  print(f'âš ï¸ Processing error: {e}')
          
          # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆæœ€æ–°50ä»¶ã¾ã§ä¿æŒã€è°·ç«¯é–¢é€£è¨˜äº‹ãªã®ã§å¤šã‚ã«ä¿å­˜ï¼‰
          all_articles = processed + previous_data
          all_articles = all_articles[:50]
          
          with open('news-data.json', 'w', encoding='utf-8') as f:
              json.dump(all_articles, f, ensure_ascii=False, indent=2)
          
          print(f'âœ… Total articles saved: {len(all_articles)} (New: {len(processed)})')
          "
          
      - name: Create HTML
        run: |
          echo "Creating HTML from AI data..."
          python3 -c "
          import json
          from datetime import datetime
          
          # Load AI-generated data
          with open('news-data.json', 'r', encoding='utf-8') as f:
              news_data = json.load(f)
          
          # Generate HTML with AI summaries
          html = '''<!DOCTYPE html>
          <html lang=\"ja\">
          <head>
            <meta charset=\"UTF-8\">
            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">
            <title>è°·ç«¯é¸æ‰‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ  - å¤§å­¦é‡çƒç‰¹åŒ–</title>
            <style>
              * { margin: 0; padding: 0; box-sizing: border-box; }
              body { 
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
                background: linear-gradient(135deg, #2E8B57 0%, #228B22 50%, #006400 100%);
                color: #333;
                line-height: 1.6;
                min-height: 100vh;
              }
              .container {
                max-width: 1200px;
                margin: 0 auto;
                padding: 20px;
              }
              header { 
                text-align: center; 
                margin-bottom: 40px;
                background: rgba(255, 255, 255, 0.95);
                padding: 30px;
                border-radius: 15px;
                box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
                backdrop-filter: blur(10px);
                border: 3px solid #228B22;
              }
              .header-title {
                font-size: 2.5rem;
                font-weight: 700;
                color: #006400;
                margin-bottom: 10px;
                text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
              }
              .header-subtitle {
                font-size: 1.2rem;
                color: #2E8B57;
                margin-bottom: 15px;
                font-weight: 600;
              }
              .player-info {
                background: #228B22;
                color: white;
                padding: 15px;
                border-radius: 10px;
                margin: 15px 0;
                font-size: 1rem;
                font-weight: 500;
              }
              .last-update {
                font-size: 0.9rem;
                color: #666;
                font-style: italic;
              }
              .news-grid {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
                gap: 25px;
                margin-bottom: 40px;
              }
              .news-card {
                background: rgba(255, 255, 255, 0.95);
                border-radius: 15px;
                padding: 25px;
                box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
                backdrop-filter: blur(10px);
                transition: all 0.3s ease;
                border-left: 5px solid #228B22;
              }
              .news-card:hover {
                transform: translateY(-5px);
                box-shadow: 0 15px 40px rgba(0, 0, 0, 0.15);
                border-left: 5px solid #006400;
              }
              .news-title {
                font-size: 1.3rem;
                font-weight: 600;
                margin-bottom: 15px;
                line-height: 1.4;
              }
              .news-title a {
                color: #006400;
                text-decoration: none;
                transition: color 0.3s ease;
              }
              .news-title a:hover {
                color: #228B22;
              }
              .news-meta {
                display: flex;
                justify-content: space-between;
                align-items: center;
                margin-bottom: 15px;
                font-size: 0.85rem;
                color: #7f8c8d;
              }
              .news-source {
                background: #228B22;
                color: white;
                padding: 6px 12px;
                border-radius: 20px;
                font-size: 0.8rem;
                font-weight: 500;
              }
              .keyword-badge {
                background: #FFD700;
                color: #006400;
                padding: 4px 8px;
                border-radius: 15px;
                font-size: 0.75rem;
                font-weight: 600;
                margin-left: 10px;
              }
              .news-date {
                font-style: italic;
                color: #666;
              }
              .news-summary {
                font-size: 1rem;
                line-height: 1.6;
                color: #333;
                margin-bottom: 15px;
                background: #f8f9fa;
                padding: 15px;
                border-radius: 8px;
                border-left: 4px solid #228B22;
              }
              .read-more {
                display: inline-block;
                background: #228B22;
                color: white;
                padding: 8px 16px;
                border-radius: 20px;
                text-decoration: none;
                font-weight: 500;
                font-size: 0.9rem;
                transition: all 0.3s ease;
              }
              .read-more:hover {
                background: #006400;
                transform: scale(1.05);
              }
              .stats {
                text-align: center;
                background: rgba(255, 255, 255, 0.95);
                padding: 20px;
                border-radius: 15px;
                box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
                backdrop-filter: blur(10px);
                margin-bottom: 30px;
                border: 2px solid #228B22;
              }
              .stats-text {
                font-size: 1.1rem;
                color: #006400;
                font-weight: 600;
              }
              .no-news {
                text-align: center;
                background: rgba(255, 255, 255, 0.95);
                padding: 40px;
                border-radius: 15px;
                color: #666;
                font-size: 1.1rem;
              }
              footer { 
                text-align: center; 
                color: rgba(255, 255, 255, 0.9);
                font-size: 0.9rem;
                margin-top: 40px;
                background: rgba(0, 0, 0, 0.2);
                padding: 20px;
                border-radius: 10px;
              }
              @media (max-width: 768px) {
                .container { padding: 15px; }
                .header-title { font-size: 2rem; }
                .news-grid { grid-template-columns: 1fr; gap: 20px; }
                .news-card { padding: 20px; }
                .news-title { font-size: 1.2rem; }
              }
            </style>
          </head>
          <body>
            <div class=\"container\">
              <header>
                <div class=\"header-title\">âš¾ è°·ç«¯é¸æ‰‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ </div>
                <div class=\"header-subtitle\">å¤§å­¦é‡çƒãƒ»ãƒ—ãƒ­é‡çƒãƒ‹ãƒ¥ãƒ¼ã‚¹ AIè¦ç´„</div>
                <div class=\"player-info\">
                  ğŸŸï¸ æ—¥æœ¬å¤§å­¦ â†’ èª­å£²ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„ | ğŸ« æ˜Ÿç¨œé«˜æ ¡å‡ºèº«
                </div>
                <div class=\"last-update\">Last updated: ''' + datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M') + '''</div>
              </header>'''
          
          # Add stats section
          total_articles = len(news_data)
          
          if total_articles == 0:
              html += '''
                  <div class=\"no-news\">
                    <h2>âš¾ è°·ç«¯é¸æ‰‹é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ä¸­...</h2>
                    <p style=\"margin-top: 15px;\">ç¾åœ¨ã€è°·ç«¯é¸æ‰‹ã«é–¢ã™ã‚‹æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</p>
                    <p style=\"margin-top: 10px;\">æ¯æœ8æ™‚ã«è‡ªå‹•æ›´æ–°ã•ã‚Œã¾ã™ã€‚</p>
                  </div>'''
          else:
              html += f'''
                  <div class=\"stats\">
                    <div class=\"stats-text\">âš¾ è°·ç«¯é¸æ‰‹é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ {total_articles} ä»¶ | ğŸ¤– AIè¦ç´„ã§é‡çƒæƒ…å ±ã‚’ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—</div>
                  </div>
                  
                  <div class=\"news-grid\">'''
              
              # Add AI-generated news items
              for item in news_data:
                  pub_date = datetime.fromisoformat(item['pubDate']).strftime('%mæœˆ%dæ—¥ %H:%M')
                  source = item.get('source', 'Baseball RSS')
                  keyword = item.get('keyword', 'è°·ç«¯')
                  category = item.get('category', 'é‡çƒ')
                  
                  # Truncate summary for better display
                  summary = item['summary']
                  if len(summary) > 200:
                      summary = summary[:200] + '...'
                  
                  html += f'''
                    <div class=\"news-card\">
                      <div class=\"news-title\">
                        <a href=\"{item['link']}\" target=\"_blank\">{item['title']}</a>
                      </div>
                      <div class=\"news-meta\">
                        <div>
                          <span class=\"news-source\">{source}</span>
                          <span class=\"keyword-badge\">{keyword}</span>
                        </div>
                        <span class=\"news-date\">{pub_date}</span>
                      </div>
                      <div class=\"news-summary\">{summary}</div>
                      <a href=\"{item['link']}\" target=\"_blank\" class=\"read-more\">ğŸ“° å…¨æ–‡ã‚’èª­ã‚€</a>
                    </div>'''
              
              html += '''</div>'''
          
          html += '''
              <footer>
                <div>âš¾ è°·ç«¯é¸æ‰‹å°‚ç”¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆ | ğŸ¤– AI-Powered Baseball News</div>
                <div style=\"margin-top: 10px; font-size: 0.8rem;\">
                  Sources: ã‚¹ãƒãƒ‹ãƒã€æ—¥åˆŠã‚¹ãƒãƒ¼ãƒ„ã€Yahoo Sportsã€NHKã€å…±åŒé€šä¿¡ ä»–
                </div>
                <div style=\"margin-top: 5px; font-size: 0.8rem;\">
                  Keywords: è°·ç«¯ + æ—¥å¤§/å¤§å­¦/æ˜Ÿç¨œ/å·¨äºº/ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„
                </div>
              </footer>
            </div>
          </body>
          </html>'''
          
          # Write HTML file
          with open('news.html', 'w', encoding='utf-8') as f:
              f.write(html)
          
          print(f'âœ“ Created HTML with {len(news_data)} AI-generated articles')
          "
          echo "âœ“ HTML generation completed"
          
      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add news.html news-data.json
          
          if ! git diff --staged --quiet; then
            git commit -m "Update news ğŸ¤–"
            git push
          else
            echo "No changes"
          fi